{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4dcf5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.066949Z",
     "start_time": "2022-09-02T20:11:54.934857Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#display max column width\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#spacy packages\n",
    "import spacy\n",
    "\n",
    "# nltk packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gensim packages\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.parsing import strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
    "from gensim.parsing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6dd503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.082877Z",
     "start_time": "2022-09-02T20:12:00.068960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom filters for Text Processing for gensim\n",
    "\n",
    "regex = r'([\\w+]+\\:\\/\\/)?([\\w\\d-]+\\.)*[\\w-]+[\\.\\:]\\w+([\\/\\?\\=\\&\\#.]?[\\w-]+)*\\/?'\n",
    "remove_urls = lambda s: re.sub(regex, '', s)\n",
    "\n",
    "convert_to_lower = lambda s: s.lower()\n",
    "\n",
    "remove_single_char = lambda s: re.sub(r'\\s+\\w{1}\\s+', '', s)\n",
    "\n",
    "# Filters to be executed in pipeline\n",
    "CLEAN_FILTERS = [\n",
    "                remove_urls,\n",
    "                strip_tags,\n",
    "                strip_numeric,\n",
    "                strip_punctuation, \n",
    "                strip_multiple_whitespaces, \n",
    "                convert_to_lower,\n",
    "                remove_stopwords,\n",
    "                remove_single_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97144721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.098583Z",
     "start_time": "2022-09-02T20:12:00.085505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method does the filtering of all the unrelevant text elements using gensim \n",
    "def gensim_clean_pipeline(document):\n",
    "    # Invoking gensim.parsing.preprocess_string method with set of filters\n",
    "    processed_words = preprocess_string(document, CLEAN_FILTERS)\n",
    "    \n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb563d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.113930Z",
     "start_time": "2022-09-02T20:12:00.099711Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk stop words\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    output = ' '.join([i for i in text if i not in stop_words])\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb977cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.702920Z",
     "start_time": "2022-09-02T20:12:00.114516Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemitization using spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts,allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]): \n",
    "\n",
    "    doc = nlp(texts) \n",
    "    output = ' '.join([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
    "    return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d31e3d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.718799Z",
     "start_time": "2022-09-02T20:12:00.706182Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemitization using nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmit(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return([lemmatizer.lemmatize(w) for w in texts])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cbba1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T20:12:00.733609Z",
     "start_time": "2022-09-02T20:12:00.722456Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_pipeline(df, columns_lst):\n",
    "    col_new = []\n",
    "    for col in columns_lst:\n",
    "        df[col+'_new'] = df[col].apply(gensim_clean_pipeline).apply(remove_stopwords).apply(lemmatization)\n",
    "        col_new.append(col+'_new')\n",
    "    return(col_new)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc42a535c417c1b15ba0223d00d3099b9b7781e15592cc58f9812e97a667838e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
